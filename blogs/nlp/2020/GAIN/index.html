<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Document-level Relation Extraction | MLNLC</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="MLNLC publication">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/assets/css/0.styles.75f5d833.css" as="style"><link rel="preload" href="/assets/js/app.9a6ffb32.js" as="script"><link rel="preload" href="/assets/js/3.b55f54c7.js" as="script"><link rel="preload" href="/assets/js/1.a60b3d0b.js" as="script"><link rel="preload" href="/assets/js/9.1a4885d4.js" as="script"><link rel="prefetch" href="/assets/js/10.948f2692.js"><link rel="prefetch" href="/assets/js/11.5d344745.js"><link rel="prefetch" href="/assets/js/12.58ef27c8.js"><link rel="prefetch" href="/assets/js/13.781b58b1.js"><link rel="prefetch" href="/assets/js/14.986132b7.js"><link rel="prefetch" href="/assets/js/4.988bff16.js"><link rel="prefetch" href="/assets/js/5.f768774e.js"><link rel="prefetch" href="/assets/js/6.c9bbc0f7.js"><link rel="prefetch" href="/assets/js/7.47de3fba.js"><link rel="prefetch" href="/assets/js/8.ca3c4ea9.js">
    <link rel="stylesheet" href="/assets/css/0.styles.75f5d833.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>MLNLC</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>MLNLC publication</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>MLNLC</span>
            
          <span data-v-4e82dffc>2017 - </span>
          2021
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/logo.png" alt="MLNLC" class="logo"> <span class="site-name">MLNLC</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/avatar.png" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    MLNLC
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>4</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>9</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Document-level Relation Extraction</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>MLNLC</span>
            
          <span data-v-4e82dffc>2017 - </span>
          2021
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">Document-level Relation Extraction</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>许润昕</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>11/20/2020</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>NLP</span><span class="tag-item" data-v-1ff7123e>Document-level</span><span class="tag-item" data-v-1ff7123e>Relation-Extraction</span><span class="tag-item" data-v-1ff7123e>EMNLP</span></i></div></div> <div class="theme-reco-content content__default"><p><strong>Double Graph Based Reasoning for Document-level Relation Extraction</strong></p> <h2 id="background"><a href="#background" class="header-anchor">#</a> Background</h2> <p>The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications. Previous methods focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, <strong>extracting relations at the document-level is necessary</strong> for a holistic understanding of knowledge in text.</p> <p><img src="/assets/img/image1.0bceec5b.png" alt="image1"></p> <h2 id="challenges"><a href="#challenges" class="header-anchor">#</a> Challenges</h2> <p>There are several major challenges in effective relation extraction at the document-level. The figure below shows an example.</p> <div align="center"><img src="/assets/img/image2.530ed160.png" width="50%" height="50%"></div> <ol><li>The subject and object entities involved in a relation may appear in different sentences, e.g., the relation between <strong>Baltimore</strong> and <strong>U.S.</strong>, as well as <strong>Eldersburg</strong> and <strong>U.S</strong>.</li> <li>The same entity may be mentioned multiple times in different sentences.</li> <li>The identification of many relations requires techniques of logical reasoning, e.g., <strong>Eldersburg</strong> belongs to <strong>U.S.</strong> because <strong>Eldersburg</strong> is located in <strong>Maryland</strong>, and <strong>Maryland</strong> belongs to <strong>U.S.</strong>.</li></ol> <h2 id="our-proposed-model-gain"><a href="#our-proposed-model-gain" class="header-anchor">#</a> Our proposed Model: GAIN</h2> <p>To tackle the challenges, we propose <strong>G</strong>raph <strong>A</strong>ggregation-and-<strong>I</strong>nference <strong>N</strong>etwork (<strong>GAIN</strong>). GAIN consists of double graph, i.e., mention-level graph and entity-level graph. Our intuitions are that: 1) Mention-level graph can model the interactions among mentions across sentences, so that global context can be better captured. 2) Entity-level graph can conduct logical reasoning for certain entity pairs over entities.</p> <p><img src="/assets/img/image3.c52ad8ea.png" alt="image3"></p> <p>Our model contains the following four modules.</p> <p><strong>Encoding module</strong>. Tokens of the document is represented as the concatenation of word embedding, entity type embedding, and entity id embedding. Then they are fed into the encoder (e.g., LSTM or BERT) to obtain the contextualized representation.</p> <p><strong>Mention-level Graph Aggregation Module</strong>. To model the document-level information and interactions among mentions, a heterogeneous mention-level graph is constructed followed by graph convolution network. The graph has two kinds of nodes: 1) <em>Mention node</em>, which refers to one specific entity mention in the document; 2) <em>Document node</em>, which aims to model the overall document information and serves as a pivot for interactions among different mentions. Three types of edges are leveraged to connect these nodes:</p> <ul><li><p><em>Intra-Entity Edge</em>: Mentions referring to the same entity are fully connected with intra-entity edges. In this way, the interaction among different mentions of the same entity could be modeled.</p></li> <li><p><em>Inter-Entity Edge</em>: Two mentions of different entities are connected with an inter-entity edge if they co-occur in a single sentence. In this way, interactions among entities could be modeled by co-occurrences of their mentions.</p></li> <li><p><em>Document Edge</em>: All mentions are connected to the document node with the document edge. With such connections, the document node can attend to all the mentions and enable interactions between document and mentions. Besides, the distance between two mention nodes is at most two with the document node as a pivot. Therefore long-distance dependency can be better modeled.</p></li></ul> <p><strong>Entity-level Graph Inference Module</strong>. To explicitly capture the logic reasoning chain of entity pairs over all the entities, we constuct an entity-level graph by merging mention nodes referring to the same entity in the mention-level graph into an entity node. Concretely, to model the logical chain between a certain entity pair, we find out all the two-hop paths between them, in which a path is represented as the concatenation of both forward and backward edges. Then we levelrage attention mechanism to aggregate multiple paths into a reasoning-aware path representation.</p> <p><strong>Classification Module</strong>. Since a pair of entities may contain multiple relations, we formulate the task as a multi-label classification. Concretely, we first concatenate the entity, document, and path representations. Then we feed it into a MLP and use sigmoid function to predict the score for all possible relations.</p> <p><img src="/assets/img/image4.8b8e6f36.png" alt="image4"></p> <h2 id="experiments"><a href="#experiments" class="header-anchor">#</a> Experiments</h2> <h3 id="dataset"><a href="#dataset" class="header-anchor">#</a> Dataset</h3> <p>We evaluate our model on DocRED (Yao et al., 2019), a large-scale human-annotated dataset for document-level RE constructed from Wikipedia and Wikidata. DocRED has 96 relations types, 132, 275 entities, and 56, 354 relational facts in total. Documents in DocRED contain about 8 sentences on average, and more than 40.7% relation facts can only be extracted from multiple sentences. Moreover, 61.1% relation instances require various inference skills such as logical inference (Yao et al., 2019). we follow the standard split of the dataset, 3, 053 documents for training, 1, 000 for development and 1, 000 for test.</p> <h3 id="main-results"><a href="#main-results" class="header-anchor">#</a> Main Results</h3> <p>We compare the performance among the following models:</p> <ul><li><strong>CNN</strong>, <strong>LSTM</strong>, <strong>BiLSTM</strong>, <strong>Context-Aware</strong>, <strong>BERT-RE</strong>, <strong>RoBERTa-RE</strong>, <strong>CorefBERT-RE</strong>, <strong>CorefRoBERTa-RE</strong>: Using different encoding mechanisms to simply encode the whole document and extract relations.</li> <li><strong>HIN-Glove</strong>, <strong>HIN-BERT</strong>: Extracting relations through a hierarchical interaction network with either Glove embedding or BERT.</li> <li><strong>GAT</strong>, <strong>GCNN</strong>, <strong>EOG</strong>, <strong>AGGCN</strong>, <strong>LSR-Glove</strong>, <strong>LSR-BERT</strong>: Previous graph-based methods, while our graph construction is totally different from theirs and they conduct logical reasoning only based on GCN.</li> <li><strong>GAIN-Glove</strong>, <strong>GAIN-BERT</strong>: Our proposed model with either Glove embedding or BERT.</li></ul> <p>The evaluation metrics we use are F1/AUC and Ign-F1/Ign-AUC. The latter means we do not consider the triples (i.e., head-relation-tail) that are already contained in the training set.</p> <p><img src="/assets/img/image5.fe0332d1.png" alt="image5"></p> <p>The key observations are:</p> <ul><li>Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9∼12.82 F1 score on the test set.</li> <li>Among the models using BERT or BERT variants, GAIN-BERT base yields a great improvement of F1/Ign F1 on dev and test set by 2.22/6.71 and 2.19/2.03, respectively, in comparison with the strong baseline LSR-BERT base. GAIN-BERT large also improves 2.85/2.63 F1/Ign F1 on test set compared with
previous state-of-the-art method, CorefRoBERTaRElarge.</li> <li>GAIN can better utilize powerful BERT representation. LSR-BERT base improves F1 by 3.83 and 4.87 on dev and test set with GloVe embedding replaced with BERTbase while our GAIN-BERT base yields an improvement by 5.93 and 6.16.</li></ul> <h3 id="ablation-study"><a href="#ablation-study" class="header-anchor">#</a> Ablation Study</h3> <p>We conduct ablation study by removing the mention-level graph, entity-level graph inference module, and the document node in the mention-level graph. The F1 scores on test set significantly decrease by 2.02~2.34/1.61~1.90 for GAIN-Glove/GAIN-BERT.</p> <p><img src="/assets/img/image6.8b6a567b.png" alt="image6"></p> <h3 id="further-analysis"><a href="#further-analysis" class="header-anchor">#</a> Further Analysis</h3> <h4 id="cross-sentence-relation-extraction"><a href="#cross-sentence-relation-extraction" class="header-anchor">#</a> Cross-sentence Relation Extraction</h4> <p>We evaluate GAIN on relations within a single sentence (Intra-F1) and those involving multiple sentences (Inter-F1), respectively. GAIN outperforms other baselines not only in Intra-F1 but also Inter-F1. The removal of Mention-level Graph (hMG) leads to a more considerable decrease in Inter-F1 than Intra-F1, which indicates
our hMG do help interactions among mentions, especially those distributed in different sentences with long-distance dependency.</p> <div align="center"><img src="/assets/img/image7.00350868.png"></div> <h4 id="logical-reasoning-for-relation-extraction"><a href="#logical-reasoning-for-relation-extraction" class="header-anchor">#</a> Logical Reasoning for Relation Extraction</h4> <p>We evaluate GAIN on relations requiring logical reasoning (Infer-F1), and the experimental results show GAIN can better handle relational inference. For example, GAIN-BERT base improves 5.11 Infer-F1 compared with RoBERTa-RE base. The inference module also plays an important role in capturing potential inference chains between entities, without which GAIN-BERT base would drop by 1.78 Infer-F1.</p> <div align="center"><img src="/assets/img/image8.9bf5e8cf.png"></div> <h3 id="case-study"><a href="#case-study" class="header-anchor">#</a> Case Study</h3> <p>The figure above shows the case study of our proposed model GAIN, in comparison with other baselines. As is shown, BiLSTM can only identify two relations within the first sentence. Both BERT-RE base and GAIN-BERT base can successfully predict <strong>Without Me</strong> is part of <strong>The Eminem Show</strong>. But only GAIN-BERT base is able to deduce the performer and publication date of <strong>Without Me</strong> are the same as those of <strong>The Eminem Show</strong>, namely <strong>Eminem</strong> and <strong>May 26, 2002</strong>, where it requires logical inference across sentences.</p> <p><img src="/assets/img/image9.4173edab.png" alt="image9"></p> <h2 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> Conclusion</h2> <p>Extracting inter-sentence relations and conducting relational reasoning are challenging in document-level relation extraction. In this paper, we introduce Graph Aggregationand-Inference Network (GAIN) to better cope with document-level relation extraction, which features double graphs in different granularity. GAIN
utilizes a heterogeneous Mention-level Graph to model the interaction among different mentions across the document and capture document-aware features. It also uses an Entity-level Graph with a proposed path reasoning mechanism to infer relations more explicitly. Experimental results on the large-scale human annotated dataset, DocRED, show GAIN outperforms previous methods, especially in inter-sentence and inferential relations scenarios. The ablation study also confirms the effectiveness of different modules in our model.</p> <h2 id="reference"><a href="#reference" class="header-anchor">#</a> Reference</h2> <ul><li>Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, Maosong Sun. 2019. DocRED: A Large-Scale Document-Level Relation Extraction Dataset. In Proceedings of ACL.</li></ul></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">4/12/2021, 1:04:43 PM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#background" class="sidebar-link reco-side-background" data-v-70334359>Background</a></li><li class="level-2" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#challenges" class="sidebar-link reco-side-challenges" data-v-70334359>Challenges</a></li><li class="level-2" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#our-proposed-model-gain" class="sidebar-link reco-side-our-proposed-model-gain" data-v-70334359>Our proposed Model: GAIN</a></li><li class="level-2" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#experiments" class="sidebar-link reco-side-experiments" data-v-70334359>Experiments</a></li><li class="level-3" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#dataset" class="sidebar-link reco-side-dataset" data-v-70334359>Dataset</a></li><li class="level-3" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#main-results" class="sidebar-link reco-side-main-results" data-v-70334359>Main Results</a></li><li class="level-3" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#ablation-study" class="sidebar-link reco-side-ablation-study" data-v-70334359>Ablation Study</a></li><li class="level-3" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#further-analysis" class="sidebar-link reco-side-further-analysis" data-v-70334359>Further Analysis</a></li><li class="level-3" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#case-study" class="sidebar-link reco-side-case-study" data-v-70334359>Case Study</a></li><li class="level-2" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#conclusion" class="sidebar-link reco-side-conclusion" data-v-70334359>Conclusion</a></li><li class="level-2" data-v-70334359><a href="/blogs/nlp/2020/GAIN/#reference" class="sidebar-link reco-side-reference" data-v-70334359>Reference</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/assets/js/app.9a6ffb32.js" defer></script><script src="/assets/js/3.b55f54c7.js" defer></script><script src="/assets/js/1.a60b3d0b.js" defer></script><script src="/assets/js/9.1a4885d4.js" defer></script>
  </body>
</html>
